{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3217f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vv/jld3_3hn5fz42wp20lz51yxm0000gn/T/ipykernel_10429/49787312.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n",
      "/var/folders/vv/jld3_3hn5fz42wp20lz51yxm0000gn/T/ipykernel_10429/49787312.py:6: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're referring to the winners of the major football tournaments in 2022!\n",
      "\n",
      "Here are some of the notable ones:\n",
      "\n",
      "1. **FIFA World Cup 2022**: The winner was Argentina, who defeated France 4-2 in a penalty shootout after the match ended 3-3 after extra time.\n",
      "2. **UEFA Champions League 2022**: The winner was Real Madrid, who defeated Liverpool 1-0 in the final.\n",
      "3. **Premier League 2021-22**: The winner was Manchester City, who finished top of the table with 93 points from 38 matches.\n",
      "4. **La Liga 2021-22**: The winner was Real Madrid, who won their 35th La Liga title with 90 points from 38 matches.\n",
      "\n",
      "Let me know if you're looking for any other specific information or tournament winners!"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    model = \"llama3.1\", callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "answer = llm.invoke(\"football winner 2022\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c0a15",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4838608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_kappa = \"./pdfs/Kappe.pdf\"\n",
    "loader = PyPDFLoader(pdf_kappa)\n",
    "\n",
    "# Load all pages and extract text content\n",
    "pages = [page.page_content for page in loader.lazy_load()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ae227",
   "metadata": {},
   "source": [
    "### Semantic chunk to split the thesis\n",
    "Semantic chunking considers the relationship within the text. It divides the text into meaningful, semantically complete chunks.\n",
    "\n",
    "Semantic chunk involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other and then grouping sentences with the most similar embeddings together.\n",
    "\n",
    "- Emebedding Models:\n",
    "    - bge-small-en: very light and dedicate for retrieval-augmented language tasks. It's designed to officially handle tasks like passage retrieval and semantic similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295b906",
   "metadata": {},
   "source": [
    "- Create langchain docs object from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "baaab20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "\n",
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chunk the text\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "            #using 'percentile' to split the text, which is based on computing \n",
    "        #all differences between sentences and then se if any differences is greater that X percentile\n",
    "    \n",
    "    embedding_model, breakpoint_threshold_type='percentile' )\n",
    "\n",
    "\n",
    "## split the text\n",
    "docs = semantic_chunker.create_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f9fdd",
   "metadata": {},
   "source": [
    "#### Vector Store\n",
    "\n",
    "Storing the chunks in Memory for efficient retreivel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "491c66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "# Prepare texts for storage \n",
    "texts = [chunk.page_content for chunk in docs]\n",
    "\n",
    "# create a vector store\n",
    "vector_store = InMemoryVectorStore.from_texts(\n",
    "    texts,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57810cf",
   "metadata": {},
   "source": [
    "Vectorstore to retreive documents based. light wrapper around the vector store class to make it conform to the retriever interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f3d3d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity\", search_kwargs={\"score_threshold\":0.5,\"k\":30} )\n",
    "\n",
    "\n",
    "user_query = \"what are the research questions of the thesis\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8653d",
   "metadata": {},
   "source": [
    "#### Prompt Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8f3d403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant helping me  preparing for my thesis defense:\n",
    "Use the content provide to answer my query:\n",
    "\n",
    "content:\n",
    "{retrieved_docs}\n",
    "\n",
    "query:\n",
    "{user_query}\n",
    "\n",
    "Provide a clear, scientific  answer based on given content. \n",
    "\n",
    "If I asked about a summary, give a coherent, high-level overview.\n",
    "\n",
    "Never include document ids or metadata in your response.\n",
    "\"\"\"\n",
    "\n",
    "# Structure of prompt\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "14efc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chain creation\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = structured_prompt |llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9ddf403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided content, we can infer that the research questions for this thesis are not explicitly stated in the given snippet. However, from Document(id='93a229a6-8d42-4498-bebc-7163a0e5e10c', metadata={}, page_content='24 3. ResearchMethodology\\napplyingartifacts( Hevneretal.'), we can see that it is related to the chapter \"Research Methodology\" which might give us a hint on what research questions are being addressed.\n",
      "\n",
      "But, from Document(id='baf3aa59-3992-40fa-b4d3-3cf8b3d9b2a8', metadata={}, page_content='. .1\\n1.2 ProblemStatement&ResearchQuestions . . . . . . . . . . . . . . . . . . .5\\n1.3 ObjectiveandContributions . . . . . . . . . . . . . . . . . . . . . . . . .''), we can see that there is a section titled \"Problem Statement & Research Questions\" which might hold the answer to our query.\n",
      "\n",
      "Unfortunately, we don't have enough content to provide a clear and accurate answer. However, from Document(id='5eb17a09-2d7a-444b-839b-7ae59bcb89cc', metadata={}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19\\n2.4.1 ThinkingandDecisionMaking . . . . . . . . . . . . . . . . . . . . .19\\n2.4.2 NudgingandDigitalNudges . . . . . . . . . . . . . . . . . . . . . .20\\n2.5 FoodRecommenderSystemsandDigitalNudges . . . . . . . . . . . . . . .21\\n2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
     ]
    }
   ],
   "source": [
    "## Invoke the chain\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"retrieved_docs\": retrieved_docs,\n",
    "    \"user_query\": user_query\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b68ae",
   "metadata": {},
   "source": [
    "###  Recursive Chunking\n",
    " We divides the input text into smaller chunks in a h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the Pdf using Unstructured.io\n",
    "## from unstructured.partition.auto import partition\n",
    "\n",
    "elements = partition(\"./pdfs/Kappe.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b912d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert element into strings\n",
    "raw_texts = [str(el) for el in elements if str(el).strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = True,\n",
    "    separators=[\"chapter\"]\n",
    ")\n",
    "\n",
    "# chunk the text\n",
    "rec_chunk = text_splitter.create_documents(raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "940f43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "# Prepare texts for storage \n",
    "Rec_texts = [chunk.page_content for chunk in rec_chunk]\n",
    "\n",
    "# create a vector store\n",
    "vector_store_rec = InMemoryVectorStore.from_texts(\n",
    "    Rec_texts,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "45a1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity\", \n",
    "                                      search_kwargs={\"score_threshold\":0.7,\"k\":30} )\n",
    "\n",
    "\n",
    "user_query = \"What are the main experiments from the thesis?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ad784f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After analyzing the provided content, it appears that there is no direct mention of \"main experiments\" in the abstract or introduction sections. However, we can infer some information about the research methodology and experiments conducted.\n",
      "\n",
      "According to Document #39024c0b-1a8f-4179-84d0-c4e889ace382 (page_content='30 3. Research Methodology'), it is mentioned that the thesis follows a conceptually grounded approach, which involves applying artifacts (Hevner et al.) as part of the research methodology.\n",
      "\n",
      "Additionally, Document #c7f76522-f74d-41e5-8921-779250ab821e (page_content='40 4. Results and Discussions') contains some references to user experiments (e.g., [29] B. P. Knijnenburg, M. C. Willemsen, Evaluating recommender systems with user experiments). However, these are not explicitly stated as the main experiments of the thesis.\n",
      "\n",
      "Based on this information, it is difficult to provide a clear and definitive answer about the main experiments of the thesis without more context or specific details about the research objectives and methodology. However, we can infer that the thesis likely involves conducting user experiments to evaluate recommender systems.\n",
      "\n",
      "To summarize:\n",
      "\n",
      "* The thesis follows a conceptually grounded approach with artifact-based research methodology.\n",
      "* User experiments are mentioned as part of the evaluation process for recommender systems.\n",
      "* More information is needed to determine the main experiments conducted in the thesis."
     ]
    }
   ],
   "source": [
    "## Invoke the chain\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"retrieved_docs\": retrieved_docs,\n",
    "    \"user_query\": user_query\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9320b96",
   "metadata": {},
   "source": [
    "## re-Build Using LLamaIndex\n",
    "\n",
    "Build a data ingestion pipeline into vector database, and then build a retrieval pipeline. Using the following Stack\n",
    "-  bge-small-en as embedding model\n",
    "- PostgresSql as the vectorstore \n",
    "- Lama3.1 as the LLm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e7b99",
   "metadata": {},
   "source": [
    "#### Upload the model\n",
    "\n",
    "- the model is already uploaded using Ollama\n",
    "- emebedding model with fastembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cc45a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vct_db\"\n",
    "host = \"localhost\"\n",
    "password = \"ayoub123\"\n",
    "port= \"5432\"\n",
    "user = \"ayoub\"\n",
    "\n",
    "\n",
    "# connect to postgresdb\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname =db_name,\n",
    "    host = host,\n",
    "    password = password,\n",
    "    port = port,\n",
    "    user = user\n",
    ")\n",
    "\n",
    "conn.autocommit = True\n",
    "\n",
    "# ## remove if exist data\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2414c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    \n",
    "    database=db_name,\n",
    "    host = host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user = user,\n",
    "    table_name=\"kappa_db\",\n",
    "    embed_dim=384\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d5352",
   "metadata": {},
   "source": [
    "### Build Data ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2582729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_kappa = \"./pdf/thesis.pdf\"\n",
    "loader = PyPDFLoader(pdf_kappa)\n",
    "\n",
    "# Load all pages and extract text content\n",
    "pages = [page.page_content for page in loader.lazy_load()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63b72fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./pdf/thesis.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "630e606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text spliter to split document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size= 1024,\n",
    ")\n",
    "\n",
    "text_chunk = []\n",
    "doc_idxs = []\n",
    "\n",
    "for doc_idx, docs in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(docs.text)\n",
    "    text_chunk.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "691cf8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct nodes from techchunk\n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunk):\n",
    "    node = TextNode(\n",
    "        text = text_chunk,\n",
    "    )\n",
    "    str_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = str_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0345ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate embedding for each node\n",
    "\n",
    "for node in nodes:\n",
    "    node_embeddings = embedding_model.embed_documents(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    \n",
    "    node.embedding = node_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b8dbb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['63441419-acc3-45c0-9377-1acc56b1a05b',\n",
       " '44866717-7a2e-423b-93be-5aafd67e607e',\n",
       " '60044aa6-2b59-44cb-b1f6-f07950faaddf',\n",
       " '6906f8d5-1a3a-4bdb-b2e5-24a915ac5536',\n",
       " '6d9d101e-b85d-4cb3-8666-892ae698bae5',\n",
       " 'aedfd3d7-ab78-440f-9085-396ca28cfec9',\n",
       " '25f9db6c-be2c-4999-90d4-166771553a67',\n",
       " 'ffd71ca7-946b-4a9e-863d-49c4351b4dd4',\n",
       " '9f97f38c-114f-4a0c-b2e8-f88ccd505ef1',\n",
       " 'aef7e0e0-8759-4f5e-8cac-b6a4a9691191',\n",
       " '58044267-e3e7-4e4b-aef8-d941b1024674',\n",
       " 'b808b1e4-ce59-472a-88a3-8d718bc92a20',\n",
       " 'ffec7f48-18a0-473f-aa78-026956a0b8c7',\n",
       " '9d967630-09c9-4147-9837-17872da85057',\n",
       " '0a92f4e3-e4a3-41bc-afe9-15d7a542d352',\n",
       " 'f4d0c0db-35a4-4eb8-8baa-eba7427812b0',\n",
       " '90372ef3-ded1-4a48-b336-fd0d50111527',\n",
       " '39c69d6c-96ea-4379-93c0-23afe5318908',\n",
       " '70933611-00e2-46da-bfc5-7dbffef034ad',\n",
       " '59f3981d-b075-4c4f-ae2e-411e80093d92',\n",
       " '98772fd3-2efa-4fbc-916b-8f6c9e3790ad',\n",
       " '1cf743d7-d1e1-4c4d-a9db-8b22b8b17e74',\n",
       " '0f63a143-fe56-449d-928f-11e409bd253d',\n",
       " 'ea2d55b8-b688-48d7-8105-4b7d7444a47c',\n",
       " '9d0a455a-4d1a-4b4f-8b7b-4038c565b2b1',\n",
       " '5391c7b4-7cf4-4360-a14f-a03bd58b61cc',\n",
       " 'f2b40ed8-36f2-4c79-b567-ccaf48e9f3fd',\n",
       " 'af6242e4-88d2-4a0b-8496-708e44cae666',\n",
       " 'b5c96e7d-9b04-4bf2-9614-f37fc5b122bd',\n",
       " '6aaa6153-7251-4d1b-afab-58dc53688cc1',\n",
       " '393deb78-eaf4-4e54-9d17-69b8bc955da7',\n",
       " 'f83a8aac-5b43-4be3-8e2e-d136fdae1528',\n",
       " 'ed86776d-d84c-4da0-a956-cfe3af81b436',\n",
       " 'da901e51-2625-452f-b713-41f7856dd9e8',\n",
       " '92e6f028-94f0-4399-be0e-b0bae7e1b86c',\n",
       " '04606491-3b66-4d83-8cff-da7dccf9fc5f',\n",
       " '98225a77-4771-4dbd-b1a7-d5907ce28999',\n",
       " '34afa2f8-0581-4069-a8f5-d82175573e3e',\n",
       " '13724526-5c5a-40ec-80d3-ebe8c01b41f1',\n",
       " 'c379db11-89bd-49fd-80ee-32197d655945',\n",
       " '87fa29c1-8dfe-482c-a1b3-6e696154eaa2',\n",
       " 'b64e1ff3-a001-4dca-92a3-262bbd06edd5',\n",
       " '99ca0092-67ac-4bc0-bed9-b4ecc0c52c14',\n",
       " 'b392e54a-b637-420b-8cb1-06f0987d8a87',\n",
       " 'ec2566af-372d-4501-bb0e-42dc9e05ed2f',\n",
       " 'fc68f090-3290-4329-b2a4-01a1cf485cd6',\n",
       " '99db0b47-9076-43a3-8b94-c917492a20f7',\n",
       " 'acf34008-40b5-4bb7-a77b-3460b9b9b131',\n",
       " '5df2857e-799f-405a-a5fe-93b6acbe5c7c',\n",
       " '58f6c11f-8a81-436d-94cf-206f9a238628',\n",
       " '18b3c63c-8dbc-4210-82eb-5bc8e757b56a',\n",
       " '36aa49da-dc67-4302-8bda-3e79034671b4',\n",
       " 'cc8b1120-382a-4f8a-8166-d7c1872f4386',\n",
       " '8acd731b-defc-472c-9794-e89b0bf64cfc',\n",
       " '8adb5b85-6bb3-4446-94d5-5a60cb023b78',\n",
       " '9a4ccfed-cca5-4909-a279-6a3361348c6c',\n",
       " 'b0ffcd16-5e45-499b-816f-c934b322f87a',\n",
       " 'a7ae62e4-1e01-45ad-a4e5-2a56e0305273',\n",
       " 'e8eed972-5cdc-4aa0-afcc-e0b5d3ab7273',\n",
       " '25e0c622-3698-48ed-a167-179c369b12f7',\n",
       " 'a90c9b50-928a-4868-99e4-5be839649801',\n",
       " '220c53c7-393d-4bf6-ab0d-42f75a867dd7',\n",
       " 'b5cd6bb9-75b4-4f2e-9bf6-bf560d9d6cf2',\n",
       " 'a4db3a72-7e52-452c-928e-be23612b6371',\n",
       " '510e0d05-877b-4bc4-8618-ec8e5ca5e12d',\n",
       " 'fa624ee0-3a3f-4562-95d8-605618de8706',\n",
       " '161acee2-6674-4043-bfd7-bd5a4a9569e2',\n",
       " 'c9b9b4ff-528a-430f-b173-682c1e16b8ef',\n",
       " '403d3f8e-6efa-406f-86a0-0a085b6bf543',\n",
       " 'd56eb077-517f-43c1-9c60-a82983f595bd',\n",
       " '377a4751-4cd5-4f2c-9848-d6dfadb9a9a2',\n",
       " '38359b7e-9d9e-4324-9a4a-de60a65bd9a0',\n",
       " '05e80b08-adaf-4c23-95f8-fc03e9825ecf',\n",
       " 'd418dee7-bb86-47a5-8d6c-7703d088b609',\n",
       " '132087e2-7b3a-40a9-a881-175141527618',\n",
       " 'a216103f-2212-473d-b749-a7aed11f3e95',\n",
       " '15c93718-7572-49ac-8752-da68e1da45ef',\n",
       " '6f097591-7de4-4c00-b69a-a55fb376705e',\n",
       " '2b203649-8ce7-4131-8b5d-7cadf812b466',\n",
       " 'a72caac8-d672-4bc7-a9af-90cf1fb2c0b1',\n",
       " 'a597a89c-9a22-4e37-ad73-7db73aa7a1f2',\n",
       " '83055a27-fbcb-438e-84ce-548b03d4e8d5',\n",
       " '435cc95b-24ab-407e-aa75-645db0afcb15',\n",
       " '7ed9b69f-0234-431f-abca-e3541f23785e',\n",
       " '7a3f60b7-98a2-410d-92cc-fd7ef582f95b',\n",
       " 'a7e4338c-f954-4402-a470-0dc443225972',\n",
       " '5c6e2e0a-c507-4775-bef8-0fec18199e81',\n",
       " '97b009e6-f3bf-4e4f-9275-77b59aa1ffec',\n",
       " '6f4c93f4-187d-4bb7-afa0-8aabb2f26743',\n",
       " '2f877a47-fdc4-4608-8044-d251e23bc9ff',\n",
       " 'ae8bee12-8e2c-4b05-a997-e30fa93f5e89',\n",
       " 'a4fd0573-929b-4eef-b1b9-b4ac6254ef84',\n",
       " 'dbe56ec3-3c74-4aba-a576-9d8e12102f84',\n",
       " 'db2913f7-f62a-4813-bd5c-bd9cd7440a9e',\n",
       " 'c891b02a-8bdc-4145-ae85-05ca43c22f4f',\n",
       " 'eb251fb5-b1fe-4e16-9766-00c3143993a0',\n",
       " '629e1515-11cd-4c6d-9fbc-ec1a604eab1a',\n",
       " '070ac424-ff0e-4bc8-98c5-23792237fd92',\n",
       " '6f45b33d-cfad-4440-bce2-5525f670005b',\n",
       " 'd35fce69-49ab-45a9-b564-ec9aebcd4518',\n",
       " '95f8b8cb-f30e-4559-a9ee-1dbe140f9183',\n",
       " '17c4dea1-32e0-4355-a059-93b9b36f9140',\n",
       " '8efc81ad-d7cd-48b4-b612-59089003a570',\n",
       " '91ca46f7-72dc-474d-a861-2f221f0f0f78',\n",
       " '75f23f54-51ea-4cd8-ae3e-a5baa9744248',\n",
       " '1e4d5eb5-9205-466f-be16-e2611f624b8f',\n",
       " '31afa595-addd-4182-b73c-7acebfce53c6',\n",
       " '4b4a1776-d876-4f30-a960-33f88eefd932',\n",
       " 'a06db33a-0696-4453-87a4-96b203bfe1df',\n",
       " 'acdfc69c-2243-428d-a034-4c6c8f5f34bb',\n",
       " 'b2d5d0cb-5c27-426e-b51d-70a7d17a4aae',\n",
       " '24b9283d-f3f5-449a-9fb2-500bbeb7060a',\n",
       " '555246bf-068d-43ec-be23-38e7f18da255',\n",
       " '8b419507-51dc-4036-9b07-9adb51d84038',\n",
       " '34f893bb-c134-4dd6-a535-b5b9b425668b',\n",
       " '6a8e48f9-1c47-41ea-9298-0505e73a368a',\n",
       " '083d6cab-d1d7-48cf-a5b1-83b1a61eb397',\n",
       " '5d54c958-f6a0-46a3-a0cd-c25a34a23d60',\n",
       " '1f754944-d7dd-4fca-9f0f-36367fdb5153',\n",
       " '013bb12b-840a-4a0c-8381-338753fc8595',\n",
       " '905d2553-3251-4186-ae18-3c9db10843dc',\n",
       " '6dbaa12e-3143-486a-9979-adbb55407705',\n",
       " 'd22446f5-9c0c-4db5-8618-40f45a81a558',\n",
       " 'a8f15c64-31e7-42bb-b2a3-fdceb5ad9304',\n",
       " '5f2f51f1-4072-4ff6-b36a-a8bf300cefe8',\n",
       " '2399ab00-8b4d-4f79-96cd-0660e32d3e99',\n",
       " 'adaaed67-0b19-450b-95dc-e138d1365a01',\n",
       " '5329a96e-c8b8-460b-8182-3629a84724f2',\n",
       " '84832445-70ef-4668-84a1-89181d00534d',\n",
       " '1698ce19-dccd-44da-8942-1c8c36e77276',\n",
       " 'f0480b18-ed17-4131-91c4-966cc9079349',\n",
       " '8986031c-93c9-49eb-a76d-b57f79214d5b',\n",
       " '4c682c75-4177-42d6-943d-9efcd1100251',\n",
       " '7b77f174-8f4a-4ac3-81dd-65994ccf7e70',\n",
       " '9ea34990-0cad-4548-a2ef-0a3574ac1223',\n",
       " '475b75ec-45ce-458a-b6db-c9613ee2304b',\n",
       " '503bf972-5f55-4065-9181-a42287b45bcc',\n",
       " '31cd70f4-1016-4996-aab0-130a988672a3',\n",
       " '75749d80-03ad-4ec5-b6e0-d0ce2c82d50f',\n",
       " '62b82ce1-ef30-4e5d-b428-a1cc62c39abd',\n",
       " 'f0fedf93-158d-478f-8316-71c77fbe5338',\n",
       " '0333aee5-7dde-4747-a466-ece5b668972a',\n",
       " '55efb708-6e26-40dc-b2e4-444ad3ee9ddd',\n",
       " '5b11ead0-8252-4581-ac4e-5c675d5a2f06',\n",
       " 'b3fe85ab-0a78-4178-b6a1-650e88123557',\n",
       " '38093d5a-f29e-4cfb-9a00-30322b45f557',\n",
       " '7025f306-0667-47f7-a999-856311e40fda',\n",
       " 'c34e8385-191b-4edb-86b1-32248e450373',\n",
       " '2b20c215-1f1a-4383-bfa1-8d8c52fa56a1',\n",
       " 'ba619d57-eddd-49af-90ef-19d82f35e120',\n",
       " '0b758681-a5b1-497b-90ca-4359f7e6afac',\n",
       " '9b0ce25c-3c55-46bf-b3eb-91da622fbad4',\n",
       " '3a04116e-df4d-439a-a9e2-addcda0bf847',\n",
       " 'aa7c6295-b4bb-4714-b6ed-2b944119c4dc',\n",
       " '21fd2c0b-4ebb-4150-a096-9acffedd8eda',\n",
       " 'eb78593a-2713-4e5b-9601-11b0e46f9308',\n",
       " '3a2873dc-9f95-4804-81a3-b3674ee0c566',\n",
       " '5805cd04-421c-4f69-b8ac-16e7d37eb712',\n",
       " '199c6250-af48-4f7a-87c4-18882abe0d8e',\n",
       " 'b9d41bf1-42ab-4f49-a6a9-f42d0d3bdb9c',\n",
       " '9b161365-37c3-4427-b6d3-fd395e8b545a',\n",
       " '92ada35c-d980-4c94-80ad-a72e0766196c',\n",
       " '55024877-e3d4-47d3-baaf-99d842fc618d',\n",
       " '4b16d284-d7eb-462d-9467-8763bbfe7397',\n",
       " 'afe7561d-cba8-4216-8609-8878747f6f68',\n",
       " '424e9445-ba60-4621-8bbc-39d33f527b68',\n",
       " 'ac36ae66-3743-465a-80d6-2fcfe27535b2',\n",
       " 'cf3a4634-ec04-4e55-976e-5e47b4dc0c20',\n",
       " 'c5c34566-17cc-4c72-9fe8-ec995347d504',\n",
       " 'aa3da242-6a56-469b-bd28-b2e2cde0970f',\n",
       " '307e7a5d-a4c1-4478-a763-7f559acc8a9f',\n",
       " '12bbc04c-1eb2-441c-890d-8f75993b59e0',\n",
       " 'c7145b12-b770-4ee8-b587-90c68b43f037',\n",
       " 'b205291f-f38a-4589-b7b9-9b0c90b23da2',\n",
       " '107b2f22-c2b6-4252-9b23-218b19921df9',\n",
       " '6c413dd7-834a-4708-bbd8-bf1699d2f6e3',\n",
       " '4451f8dc-2ebd-4e5f-9e28-7e5210c0971c',\n",
       " '9bbe9379-547c-4461-b0c3-b80ac4b27d22',\n",
       " '39acfc95-dd78-4a4c-b67a-060c14cc27b8',\n",
       " '3bab06ba-98b8-4f60-9229-a040e091d1b0',\n",
       " '713606ca-9584-49d2-b708-98273d276888',\n",
       " '35bef8de-bc7f-4779-985b-e072cb67174b',\n",
       " '2cc5a9c8-945c-49e2-a92e-fd720577175a',\n",
       " '5d174f47-50d0-4656-b5dd-98e7e72fbc86',\n",
       " '832c41f6-0350-4a29-ae88-10a3a0bb869f',\n",
       " '8920dc18-d76f-41d6-a2d8-58e957c2bd07']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load nodes into vector store\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf3611",
   "metadata": {},
   "source": [
    "### Build Retrieval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db63a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"what is the thesis about\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750236da",
   "metadata": {},
   "source": [
    "- Generate a query embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "361747e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = embedding_model.embed_query(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1256e85",
   "metadata": {},
   "source": [
    "- Query the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "77a8201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1. Introduction\n"
     ]
    }
   ],
   "source": [
    "# construct vector store query\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "\n",
    "query_mode = \"default\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embeddings,\n",
    "    similarity_top_k=20,\n",
    "    mode=query_mode   \n",
    "    \n",
    ")\n",
    "\n",
    "# return a vectorstorequeryresults\n",
    "query_results = vector_store.query(vector_store_query)\n",
    "print(query_results.nodes[1].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83316f",
   "metadata": {},
   "source": [
    "- Parse results into a set of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5fbb0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_results.nodes):\n",
    "    score : Optional[float] = None\n",
    "    if query_results.similarities is not None:\n",
    "        score = query_results.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node = node, score = score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694740a0",
   "metadata": {},
   "source": [
    "- Put into a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aea920f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k : int = 20\n",
    "    )-> None:\n",
    "        \"\"\"Init parameter \"\"\"\n",
    "        self._vector_Store = vector_store,\n",
    "        self._embed_model = embedding_model,\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "        \n",
    "    def _retrieve(self, query_bundel: QueryBundle)->List[NodeWithScore]:\n",
    "        \"\"\"Retriever\"\"\"\n",
    "        query_embeddings = embedding_model.embed_query(\n",
    "            query_bundel.query_str\n",
    "        )\n",
    "        \n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embeddings,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode = self._query_mode,\n",
    "        )\n",
    "        query_results = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        \n",
    "        for index, node in enumerate(query_results.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_results.similarities is not None:\n",
    "                score = query_results.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5553cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorRetriever(\n",
    "    vector_store,\n",
    "    embedding_model,\n",
    "    query_mode=\"default\",\n",
    "    similarity_top_k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd534235",
   "metadata": {},
   "source": [
    "- Query Engine to systhesize the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e7e31cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "llm_ol = Ollama(\n",
    "    model = \"llama3.1\",\n",
    "    request_timeout = 120,\n",
    "    context_window=8000\n",
    "    )\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm_ol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8c2f27f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 16:30:38,707 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint-based recommenders tend to perform better in promoting healthier options, especially for users with a low level of health consciousness. This is because they allow users to interact with recipe features, making it easier for them to locate recipes that meet their specific needs. However, the effectiveness of this approach can vary depending on the user's level of experience and familiarity with nutritional information.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"which recommendation approach leads to more healthier food choices?\"\n",
    "response = query_engine.query(query_str)\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235d9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
