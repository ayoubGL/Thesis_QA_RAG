{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vv/jld3_3hn5fz42wp20lz51yxm0000gn/T/ipykernel_98973/49787312.py:6: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're referring to the winners of various football competitions in 2022!\n",
      "\n",
      "Here are some notable ones:\n",
      "\n",
      "1. **FIFA World Cup Qatar 2022**:\n",
      "\t* Winners: Argentina (won against France in the final on December 18, 2022)\n",
      "\t* Golden Ball (best player): Lionel Messi\n",
      "\t* Golden Glove (best goalkeeper): Emiliano Mart√≠nez\n",
      "2. **UEFA Champions League 2021-22**:\n",
      "\t* Winners: Real Madrid (beat Liverpool 1-0 in the final on May 28, 2022)\n",
      "3. **English Premier League 2021-22**:\n",
      "\t* Winners: Manchester City\n",
      "4. **La Liga 2021-22**:\n",
      "\t* Winners: Barcelona (on points average, as the team finished with the same number of points but fewer goals conceded than Real Madrid)\n",
      "5. **Bundesliga 2021-22**:\n",
      "\t* Winners: Bayern Munich\n",
      "6. **Serie A 2021-22**:\n",
      "\t* Winners: AC Milan\n",
      "7. **UEFA Europa League 2021-22**:\n",
      "\t* Winners: Eintracht Frankfurt (beat Rangers 2-1 in the final on May 18, 2022)\n",
      "8. **Copa Libertadores 2022**:\n",
      "\t* Winners: Independiente del Valle (from Ecuador)\n",
      "\n",
      "These are just a few examples of football winners from 2022. If you're interested in more information or specific competitions, feel free to ask!"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "llm = Ollama(\n",
    "    model = \"llama3.1\", callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "answer = llm.invoke(\"football winner 2022\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c0a15",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4838608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_kappa = \"./pdfs/Kappe.pdf\"\n",
    "loader = PyPDFLoader(pdf_kappa)\n",
    "\n",
    "# Load all pages and extract text content\n",
    "pages = [page.page_content for page in loader.lazy_load()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ae227",
   "metadata": {},
   "source": [
    "### Semantic chunk to split the thesis\n",
    "Semantic chunking considers the relationship within the text. It divides the text into meaningful, semantically complete chunks.\n",
    "\n",
    "Semantic chunk involves taking the embeddings of every sentence in the document, comparing the similarity of all sentences with each other and then grouping sentences with the most similar embeddings together.\n",
    "\n",
    "- Emebedding Models:\n",
    "    - bge-small-en: very light and dedicate for retrieval-augmented language tasks. It's designed to officially handle tasks like passage retrieval and semantic similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295b906",
   "metadata": {},
   "source": [
    "- Create langchain docs object from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baaab20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c407b3e7aa14b078f58a7e6a02632ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50b4e733c444780baaf4ce451292b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f869b829f2147caab787012be2b038a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0e38f224541948cb52cc294ba221b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f142e70f5e884673a42b499a14c180ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53324cbd7d30480e8d7b39093d673f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "\n",
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chunk the text\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "            #using 'percentile' to split the text, which is based on computing \n",
    "        #all differences between sentences and then se if any differences is greater that X percentile\n",
    "    \n",
    "    embedding_model, breakpoint_threshold_type='percentile' )\n",
    "\n",
    "\n",
    "## split the text\n",
    "docs = semantic_chunker.create_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f9fdd",
   "metadata": {},
   "source": [
    "#### Vector Store\n",
    "\n",
    "Storing the chunks in Memory for efficient retreivel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "491c66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "# Prepare texts for storage \n",
    "texts = [chunk.page_content for chunk in docs]\n",
    "\n",
    "# create a vector store\n",
    "vector_store = InMemoryVectorStore.from_texts(\n",
    "    texts,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57810cf",
   "metadata": {},
   "source": [
    "Vectorstore to retreive documents based. light wrapper around the vector store class to make it conform to the retriever interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f3d3d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity\", search_kwargs={\"score_threshold\":0.5,\"k\":30} )\n",
    "\n",
    "\n",
    "user_query = \"what are the research questions of the thesis\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8653d",
   "metadata": {},
   "source": [
    "#### Prompt Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8f3d403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant helping me  preparing for my thesis defense:\n",
    "Use the content provide to answer my query:\n",
    "\n",
    "content:\n",
    "{retrieved_docs}\n",
    "\n",
    "query:\n",
    "{user_query}\n",
    "\n",
    "Provide a clear, scientific  answer based on given content. \n",
    "\n",
    "If I asked about a summary, give a coherent, high-level overview.\n",
    "\n",
    "Never include document ids or metadata in your response.\n",
    "\"\"\"\n",
    "\n",
    "# Structure of prompt\n",
    "\n",
    "structured_prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "14efc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chain creation\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = structured_prompt |llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9ddf403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided content, we can infer that the research questions for this thesis are not explicitly stated in the given snippet. However, from Document(id='93a229a6-8d42-4498-bebc-7163a0e5e10c', metadata={}, page_content='24 3. ResearchMethodology\\napplyingartifacts( Hevneretal.'), we can see that it is related to the chapter \"Research Methodology\" which might give us a hint on what research questions are being addressed.\n",
      "\n",
      "But, from Document(id='baf3aa59-3992-40fa-b4d3-3cf8b3d9b2a8', metadata={}, page_content='. .1\\n1.2 ProblemStatement&ResearchQuestions . . . . . . . . . . . . . . . . . . .5\\n1.3 ObjectiveandContributions . . . . . . . . . . . . . . . . . . . . . . . . .''), we can see that there is a section titled \"Problem Statement & Research Questions\" which might hold the answer to our query.\n",
      "\n",
      "Unfortunately, we don't have enough content to provide a clear and accurate answer. However, from Document(id='5eb17a09-2d7a-444b-839b-7ae59bcb89cc', metadata={}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .19\\n2.4.1 ThinkingandDecisionMaking . . . . . . . . . . . . . . . . . . . . .19\\n2.4.2 NudgingandDigitalNudges . . . . . . . . . . . . . . . . . . . . . .20\\n2.5 FoodRecommenderSystemsandDigitalNudges . . . . . . . . . . . . . . .21\\n2.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
     ]
    }
   ],
   "source": [
    "## Invoke the chain\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"retrieved_docs\": retrieved_docs,\n",
    "    \"user_query\": user_query\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b68ae",
   "metadata": {},
   "source": [
    "###  Recursive Chunking\n",
    " We divides the input text into smaller chunks in a h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e439d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse the Pdf using Unstructured.io\n",
    "## from unstructured.partition.auto import partition\n",
    "\n",
    "elements = partition(\"./pdfs/Kappe.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b912d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert element into strings\n",
    "raw_texts = [str(el) for el in elements if str(el).strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = True,\n",
    "    separators=[\"chapter\"]\n",
    ")\n",
    "\n",
    "# chunk the text\n",
    "rec_chunk = text_splitter.create_documents(raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "940f43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "# Prepare texts for storage \n",
    "Rec_texts = [chunk.page_content for chunk in rec_chunk]\n",
    "\n",
    "# create a vector store\n",
    "vector_store_rec = InMemoryVectorStore.from_texts(\n",
    "    Rec_texts,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "45a1cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity\", \n",
    "                                      search_kwargs={\"score_threshold\":0.7,\"k\":30} )\n",
    "\n",
    "\n",
    "user_query = \"What are the main experiments from the thesis?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ad784f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After analyzing the provided content, it appears that there is no direct mention of \"main experiments\" in the abstract or introduction sections. However, we can infer some information about the research methodology and experiments conducted.\n",
      "\n",
      "According to Document #39024c0b-1a8f-4179-84d0-c4e889ace382 (page_content='30 3. Research Methodology'), it is mentioned that the thesis follows a conceptually grounded approach, which involves applying artifacts (Hevner et al.) as part of the research methodology.\n",
      "\n",
      "Additionally, Document #c7f76522-f74d-41e5-8921-779250ab821e (page_content='40 4. Results and Discussions') contains some references to user experiments (e.g., [29] B. P. Knijnenburg, M. C. Willemsen, Evaluating recommender systems with user experiments). However, these are not explicitly stated as the main experiments of the thesis.\n",
      "\n",
      "Based on this information, it is difficult to provide a clear and definitive answer about the main experiments of the thesis without more context or specific details about the research objectives and methodology. However, we can infer that the thesis likely involves conducting user experiments to evaluate recommender systems.\n",
      "\n",
      "To summarize:\n",
      "\n",
      "* The thesis follows a conceptually grounded approach with artifact-based research methodology.\n",
      "* User experiments are mentioned as part of the evaluation process for recommender systems.\n",
      "* More information is needed to determine the main experiments conducted in the thesis."
     ]
    }
   ],
   "source": [
    "## Invoke the chain\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"retrieved_docs\": retrieved_docs,\n",
    "    \"user_query\": user_query\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9320b96",
   "metadata": {},
   "source": [
    "## re-Build Using LLamaIndex\n",
    "\n",
    "Build a data ingestion pipeline into vector database, and then build a retrieval pipeline. Using the following Stack\n",
    "-  bge-small-en as embedding model\n",
    "- PostgresSql as the vectorstore \n",
    "- Lama3.1 as the LLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c21e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-readers-file) (4.13.5)\n",
      "Collecting defusedxml>=0.7.1 (from llama-index-readers-file)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting llama-index-core<0.15,>=0.13.0 (from llama-index-readers-file)\n",
      "  Downloading llama_index_core-0.14.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pandas<2.3.0 (from llama-index-readers-file)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-macosx_10_9_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-readers-file) (6.0.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (4.14.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.12.13)\n",
      "Collecting aiosqlite (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.2.18)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.3.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.28.1)\n",
      "Collecting llama-index-workflows<3,>=2 (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading llama_index_workflows-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.2.6)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (11.2.1)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.4.54)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.20.1)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.1.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading llama_index_instrumentation-0.4.1-py3-none-any.whl.metadata (252 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.10)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (2025.6.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (25.0)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-readers-file) (3.0.2)\n",
      "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
      "Downloading llama_index_core-0.14.2-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_workflows-2.2.2-py3-none-any.whl (56 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-macosx_10_9_x86_64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_instrumentation-0.4.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, pymupdf, defusedxml, colorama, aiosqlite, pandas, griffe, llama-index-instrumentation, banks, llama-index-workflows, llama-index-core, llama-index-readers-file\n",
      "\u001b[2K  Attempting uninstall: pandas0m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/13\u001b[0m [aiosqlite]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.0‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 5/13\u001b[0m [aiosqlite]\n",
      "\u001b[2K    Uninstalling pandas-2.3.0:\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/13\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.3.00m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/13\u001b[0m [pandas]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/13\u001b[0m [llama-index-readers-file]ndex-readers-file]on]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiosqlite-0.21.0 banks-2.2.0 colorama-0.4.6 defusedxml-0.7.1 dirtyjson-1.0.8 griffe-1.14.0 llama-index-core-0.14.2 llama-index-instrumentation-0.4.1 llama-index-readers-file-0.5.4 llama-index-workflows-2.2.2 pandas-2.2.3 pymupdf-1.26.4 striprtf-0.0.26\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-readers-file pymupdf\n",
    "%pip install llama-index-vector-stores-postgres\n",
    "%pip install llama-index-embeddings-huggingface\n",
    "%pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "50eb94e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-llama-cpp\n",
      "  Downloading llama_index_llms_llama_cpp-0.5.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting llama-cpp-python<0.4,>=0.3.0 (from llama-index-llms-llama-cpp)\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: llama-index-core<0.15,>=0.13.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-llms-llama-cpp) (0.14.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (2.2.6)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.1.6)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.12.13)\n",
      "Requirement already satisfied: aiosqlite in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.3.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.2.2)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (11.2.1)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.4.54)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.20.1)\n",
      "Requirement already satisfied: griffe in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.14.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python<0.4,>=0.3.0->llama-index-llms-llama-cpp) (3.0.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (2025.6.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (25.0)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.4.6)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.0->llama-index-llms-llama-cpp) (1.3.1)\n",
      "Downloading llama_index_llms_llama_cpp-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp310-cp310-macosx_15_0_x86_64.whl size=3983551 sha256=f6c0b6658de5ead68584103977054568f11d22ea797e36e001bc9821ba10f4ef\n",
      "  Stored in directory: /Users/ayoub/Library/Caches/pip/wheels/15/ef/cc/62bb839b7bcfb604f2592a9009ef99ad878cfe39617cde3fd2\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python, llama-index-llms-llama-cpp\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [llama-index-llms-llama-cpp]ex-llms-llama-cpp]\n",
      "\u001b[1A\u001b[2KSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.16 llama-index-llms-llama-cpp-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-llama-cpp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e7b99",
   "metadata": {},
   "source": [
    "#### Upload the model\n",
    "\n",
    "- the model is already uploaded using Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d27b88dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (2.9.10)\n",
      "Requirement already satisfied: pgvector in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: asyncpg in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (0.30.0)\n",
      "Requirement already satisfied: greenlet in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (3.2.3)\n",
      "Requirement already satisfied: sqlalchemy[asyncio] in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (1.4.54)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from pgvector) (2.2.6)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/anaconda3/envs/ML_env/lib/python3.10/site-packages (from asyncpg) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "#### Initialize Postgres\n",
    "!pip install psycopg2-binary pgvector asyncpg \"sqlalchemy[asyncio]\" greenlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cc45a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "db_name = \"vct_db\"\n",
    "host = \"localhost\"\n",
    "password = \"ayoub123\"\n",
    "port= \"5432\"\n",
    "user = \"ayoub\"\n",
    "\n",
    "\n",
    "# connect to postgresdb\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname =db_name,\n",
    "    host = host,\n",
    "    password = password,\n",
    "    port = port,\n",
    "    user = user\n",
    ")\n",
    "\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b1f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
